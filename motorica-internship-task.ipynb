{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65926,"databundleVersionId":7264814,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## –ó–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∫–æ–º–∞–Ω–¥ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –æ—Ç –æ–ø—Ç–æ–º–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞—Ç—á–∏–∫–æ–≤","metadata":{"tags":[]}},{"cell_type":"markdown","source":"–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∂–µ—Å—Ç–æ–≤ –≤ –ø–∞—Ä–∞–¥–∏–≥–º–µ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∞ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation</center></h2>\n\n* [0. Installation of libraries](#0)\n* [1. Basic Data Overview](#1)\n* [2. Preparation of data](#2)\n* [3. Predicting models](#3)\n* [4. Optimization of hyperparameters](#4)\n* [5. Prediction of movements in test data](#5)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a>\n<h2 style='background:blue; border:0; color:white'><center>0. üõ† Installation of libraries</center><h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport plotly \nimport plotly.express as px\n\nfrom  sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing  import LabelEncoder\nfrom sklearn import linear_model \nfrom sklearn import tree \nfrom sklearn import ensemble \nfrom sklearn import metrics \nfrom sklearn import preprocessing \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:blue; border:0; color:white'><center>1. üìö Basic Data Overview</center><h2>","metadata":{}},{"cell_type":"code","source":"X_train = np.load('/kaggle/input/motorica-skillfactory-internship-test-task-2023-12/X_train.npy')\ndisplay('X_train:',X_train.shape)\n\nX_test = np.load('/kaggle/input/motorica-skillfactory-internship-test-task-2023-12/X_test.npy')\ndisplay('X_test:',X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`y_train.csv` —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª–∞—Å—Å—ã: \n  - `0` - –∫–æ–º–∞–Ω–¥–∞ \"Open\";\n  - `1` - –∫–æ–º–∞–Ω–¥–∞ \"—Å–≥–∏–± –º–∏–∑–∏–Ω—Ü–∞\";\n  - `2` - –∫–æ–º–∞–Ω–¥–∞ \"—Å–≥–∏–± –±–µ–∑—ã–º—è–Ω–Ω–æ–≥–æ –ø–∞–ª—å—Ü–∞\";\n  - `3` - –∫–æ–º–∞–Ω–¥–∞ \"—Å–≥–∏–± —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–∞–ª—å—Ü–∞\";\n  - `4` - –∫–æ–º–∞–Ω–¥–∞ \"–∂–µ—Å—Ç –ø–∏—Å—Ç–æ–ª–µ—Ç\";\n  - `5` - –∫–æ–º–∞–Ω–¥–∞ \"—Å–≥–∏–± —É–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞–ª—å—Ü–∞\";\n  - `6` - –∫–æ–º–∞–Ω–¥–∞ \"—Å–≥–∏–± –±–æ–ª—å—à–æ–≥–∞ –ø–∞–ª—å—Ü–∞\";\n  - `7` - –∫–æ–º–∞–Ω–¥–∞ \"–∂–µ—Å—Ç –û–ö\";\n  - `8` - –∫–æ–º–∞–Ω–¥–∞ \"–∂–µ—Å—Ç grab\";¬∂","metadata":{}},{"cell_type":"code","source":"y_train_df = pd.read_csv('/kaggle/input/motorica-skillfactory-internship-test-task-2023-12/y_train.csv')\ndisplay('y_train:',y_train_df.shape)\ndisplay(y_train_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train_df.copy()\ny_train[['sample_id', 'timestep']] = y_train['sample-timestep'].str.split('-', n=1, expand=True).astype(int)\ndisplay(y_train.head(3))\ny_train = y_train.pivot(index='sample_id', columns='timestep', values='class')\ndisplay(y_train)\ny_train_index = y_train.index\ny_train = y_train.sort_index() # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π\ny_train = y_train.values\n\nprint(y_train.shape)\ny_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 21\nfig, axx = plt.subplots(2, 1, sharex=True)\n    \naxx[0].plot(X_train[i].T)\naxx[0].set_title('OMG')\n\naxx[1].plot(y_train[i])\naxx[1].set_title('Class/Command')\naxx[1].set_xlabel('Timestep')\naxx[1].set_yticks(\n    np.arange(9),\n    ['Open', 'Pinky', 'Ring', 'Middle', 'Pistol', 'Index', 'Thumb', 'OK', 'Grab']\n)\n\nplt.suptitle(f'Train {i}')\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creation of training df","metadata":{}},{"cell_type":"code","source":"# Get the dimensions\nnum_observations, num_features, num_timestep = X_train.shape\n\n# Reshape the array to have one row per observation-timestamp combination\nreshaped_X_train = X_train.reshape((num_observations * num_timestep, num_features))\n\n# Create a DataFrame\ncolumns = [f'feature_{i}' for i in range(num_features)]\nX_train_df = pd.DataFrame(reshaped_X_train, columns=columns)\n\n# Add sample_id and timestamp columns\nX_train_df['sample_id'] = np.repeat(np.arange(num_observations), num_timestep)\nX_train_df['timestep'] = np.tile(np.arange(num_timestep), num_observations)\n\n# Reorder columns\nX_train_df = X_train_df[['sample_id', 'timestep'] + columns]\n# Display the DataFrame\nprint(X_train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_df[X_train_df['sample_id']==0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_df[['sample_id', 'timestep']] = y_train_df['sample-timestep'].str.split('-', n=1, expand=True).astype(int)\ntrain_df = X_train_df.merge(y_train_df, on=['sample_id', 'timestep'], how='outer')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_sample = train_df.sample_id.unique()\n\nfig, axx = plt.subplots(4, 2, sharex=True, figsize=(12, 16))\nfor i, ax in zip(unique_sample[:8], axx.flatten()):\n    temp = train_df[train_df['sample_id'] == i]\n    ax.plot(y_train[i])\n    ax.set_title(f'Train {i}')\n    ax.set_xlabel('Timestep')\n    ax.set_yticks(\n        np.arange(9),\n        ['Open', 'Pinky', 'Ring', 'Middle', 'Pistol', 'Index', 'Thumb', 'OK', 'Grab']\n    )\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:blue; border:0; color:white'><center>2. üóÇ Preparation of data </center><h2>","metadata":{}},{"cell_type":"code","source":"#For a sequence classification problem\n# where you want to predict a class for each time step, \n# the shapes of X_train and y_train should be as follows:\n# X_train: (number_of_samples, n_timesteps,n_features)\n# y_train: (number_of_samples, n_timesteps,n_classes)\nfrom keras.utils import to_categorical\nn_timesteps=100\nn_features=40\nn_classes=9\n\nX_train = np.load('/kaggle/input/motorica-skillfactory-internship-test-task-2023-12/X_train.npy')\nX_train = X_train.reshape((X_train.shape[0], n_timesteps, n_features))\n\ny_train = y_train_df.copy()\ny_train[['sample_id', 'timestep']] = y_train['sample-timestep'].str.split('-', n=1, expand=True).astype(int)\ny_train = y_train.pivot(index='sample_id', columns='timestep', values='class')\ny_train_index = y_train.index\ny_train = y_train.sort_index() # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π\ny_train = y_train.values\n\ny_train_encoded = to_categorical(y_train)\n# Reshape y_train_encoded to have 100 time steps\ny_train = y_train_encoded.reshape((y_train_encoded.shape[0], n_timesteps, n_classes,))\n\nprint(X_train.shape,\n      y_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Assuming X_train and X_test are your NumPy arrays with shape (number_of_samples, 40, 100)\nscaler = MinMaxScaler()\n\n# Reshape X_train to (number_of_samples * 100, 40) before scaling\nX_train_reshaped = X_train.reshape(-1, 40)\n# Fit and transform the training data\nX_train_norm_reshaped = scaler.fit_transform(X_train_reshaped)\n# Reshape it back to the original shape\nX_train_norm = X_train_norm_reshaped.reshape(X_train.shape)\n\n# Reshape X_test to (number_of_samples * 100, 40) before scaling\nX_valid_reshaped = X_valid.reshape(-1, 40)\n# Transform the test data using the same scaler\nX_valid_norm_reshaped = scaler.transform(X_valid_reshaped)\n# Reshape it back to the original shape\nX_valid_norm = X_valid_norm_reshaped.reshape(X_valid.shape)\n\n# Display the shapes\ndisplay('X_train:',X_train_norm.shape, \n        'X_test:', X_valid_norm.shape,\n       \n       'y_train:', y_train.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:blue; border:0; color:white'><center>3. üíª Predicting models </center><h2>","metadata":{}},{"cell_type":"code","source":"#Plot for accuracy and val_loss\ndef plot_accur(history, epochs=20):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs_range = range(epochs)\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense,BatchNormalization\nfrom keras.layers import TimeDistributed\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\n# Bidirectional layers can allow the model to learn from both past and future time steps.\nfrom keras.layers import Bidirectional\nfrom keras.regularizers import l1\nfrom keras.layers import GRU\n\n\nmodel_lstm = Sequential()\nmodel_lstm.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(n_timesteps, n_features)))\nmodel_lstm.add(BatchNormalization())\nmodel_lstm.add(Bidirectional(LSTM(50, return_sequences=True)))\nmodel_lstm.add(BatchNormalization())\nmodel_lstm.add(Bidirectional(LSTM(50, return_sequences=True)))\nmodel_lstm.add(BatchNormalization())\nmodel_lstm.add(Dropout(0.2))\nmodel_lstm.add(TimeDistributed(Dense(n_classes, activation='softmax')))\noptimizer = Adam(learning_rate=0.01)\nmodel_lstm.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nhistory_lstm = model_lstm.fit(X_train_norm, \n                    y_train, \n                    epochs=40, \n                    batch_size=32, \n                    validation_data=(X_valid_norm, y_valid))\nplot_accur(history_lstm, epochs=40)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gated Recurrent Unit (GRU) ","metadata":{}},{"cell_type":"code","source":"model_gru = Sequential()\nmodel_gru.add(Bidirectional(GRU(100, return_sequences=True,\n                            kernel_regularizer=l1(0.01)), \n                        input_shape=(n_timesteps, n_features)\n                       ))\nmodel_gru.add(BatchNormalization())\nmodel_gru.add(Bidirectional(GRU(50, return_sequences=True)))\nmodel_gru.add(BatchNormalization())\nmodel_gru.add(Bidirectional(GRU(50, return_sequences=True)))\nmodel_gru.add(BatchNormalization())\nmodel_gru.add(Bidirectional(GRU(25, return_sequences=True)))\nmodel_gru.add(BatchNormalization())\nmodel_gru.add(Dropout(0.2))\nmodel_gru.add(TimeDistributed(Dense(n_classes, activation='softmax')))\noptimizer = Adam(learning_rate=0.01)\nmodel_gru.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nhistory_gru = model_gru.fit(X_train_norm, \n                    y_train, \n                    epochs=40, \n                    batch_size=32, \n                    validation_data=(X_valid_norm, y_valid))\nplot_accur(history_gru, epochs=40)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style='background:blue; border:0; color:white'><center>4. üîé Optimization of hyperparameters </center><h2>","metadata":{}},{"cell_type":"code","source":"#pip install keras scikit-learn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install scikeras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional, GRU, BatchNormalization, Dropout, TimeDistributed, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l1\n\nclass KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n    def __init__(self, n_timesteps, n_features, n_classes, \n                 units1=100, units2=50, units3=50,\n                 l1_reg=0.01, dropout=0.2, learning_rate=0.001, \n                 epochs=10, batch_size=32, verbose=0):\n        self.n_timesteps = n_timesteps\n        self.n_features = n_features\n        self.n_classes = n_classes\n        self.units1 = units1\n        self.units2 = units2\n        self.units3 = units3\n        self.l1_reg = l1_reg\n        self.dropout = dropout\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.model = self.create_model()\n\n    def create_model(self):\n        model = Sequential()\n        model.add(Bidirectional(GRU(self.units1, return_sequences=True, kernel_regularizer=l1(self.l1_reg)),\n                                input_shape=(self.n_timesteps, self.n_features)))\n        model.add(BatchNormalization())\n        model.add(Bidirectional(GRU(self.units2, return_sequences=True)))\n        model.add(BatchNormalization())\n        model.add(Bidirectional(GRU(self.units3, return_sequences=True)))\n        model.add(BatchNormalization())\n        model.add(Dropout(self.dropout))\n        model.add(TimeDistributed(Dense(self.n_classes, activation='softmax')))\n        optimizer = Adam(learning_rate=self.learning_rate)\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        return model\n\n    def fit(self, X, y, validation_data=None):\n        history = self.model.fit(X, y, \n                                 epochs=self.epochs, \n                                 batch_size=self.batch_size, \n                                 verbose=self.verbose,\n                                validation_data=(X_valid, y_valid))\n        return history\n\n    def predict(self, X):\n        return self.model.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom tensorflow.keras.utils import to_categorical\n\n# Define the hyperparameter search space\nparam_dist = {\n    'units1': randint(50, 200),\n    'units2': randint(0,50),\n    'units3': randint(0,50),\n    'l1_reg': [0.0, 0.01, 0.1],  # L1 regularization parameter\n    'dropout': [0.2, 0.3, 0.4],  # Dropout rate\n    'learning_rate': [0.001, 0.01, 0.1],  # Learning rate\n    'epochs': randint(10,50)\n}\n\n# Create an instance of KerasClassifierWrapper\nwrapper = KerasClassifierWrapper(n_timesteps, n_features, n_classes)\n\n# Use RandomizedSearchCV for hyperparameter search\nrandom_search = RandomizedSearchCV(estimator=wrapper, \n                                   param_distributions=param_dist, \n                                   n_iter=10, \n                                   cv=3, \n                                   verbose=2)\n\nrandom_search.fit(X_train_norm, y_train,\n                 validation_data=(X_valid_norm, y_valid))\n\n# Get the best parameters\nbest_params = random_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best Hyperparameters: {'dropout': 0.4, 'epochs': 47, 'l1_reg': 0.1, 'learning_rate': 0.01, 'units1': 96, 'units2': 6, 'units3': 37}\nbest_model = KerasClassifierWrapper(n_timesteps, n_features, n_classes, **best_params)\nhistory_bm = best_model.fit(X_train_norm, y_train,\n                        validation_data=(X_valid_norm, y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access training and validation accuracy and loss\ntraining_accuracy = history.history['accuracy']\nvalidation_accuracy = history.history['val_accuracy']\n\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\n# Plot the accuracy and loss curves\nplot_accur(history, epochs=len(training_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h2 style='background:blue; border:0; color:white'><center>5. üîÆ Prediction of movements in test data </center><h2>","metadata":{}},{"cell_type":"code","source":"#For a sequence classification problem\n# where you want to predict a class for each time step, \n# the shapes of X_train and y_train should be as follows:\n# X_train: (number_of_samples, n_timesteps,n_features)\n# y_train: (number_of_samples, n_timesteps,n_classes)\nfrom keras.utils import to_categorical\nn_timesteps=100\nn_features=40\nn_classes=9\n\nX_test = np.load('/kaggle/input/motorica-skillfactory-internship-test-task-2023-12/X_test.npy')\nX_test = X_test.reshape((X_test.shape[0], n_timesteps, n_features))\n\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model_lstm.predict(X_test)\nprint(predictions.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat_predictions = predictions.reshape(-1, n_classes)\nflat_indices = np.array([[f\"{i}-{j}\" for j in range(n_timesteps)] for i in range(predictions.shape[0])]).reshape(-1)\n\n# Create a DataFrame\ndf_predictions = pd.DataFrame(data=flat_predictions, columns=[f\"class_{i}\" for i in range(n_classes)])\n\n# Add \"sample-timestep\" column\ndf_predictions[\"sample-timestep\"] = flat_indices\n\n# Reorder columns\ndf_predictions = df_predictions[[\"sample-timestep\"] + [f\"class_{i}\" for i in range(n_classes)]]\n\n# Melt the DataFrame to have only two columns\ndf_predictions = pd.melt(df_predictions, id_vars=[\"sample-timestep\"], var_name=\"class\", value_name=\"probability\")\n\n(df_predictions.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = df_predictions.drop('probability', axis=1)\nsubmission.to_csv('/kaggle/working/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}